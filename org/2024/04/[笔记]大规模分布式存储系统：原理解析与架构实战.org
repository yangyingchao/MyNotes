#+TITLE:  [笔记]大规模分布式存储系统：原理解析与架构实战
#+AUTHOR: Yang Yingchao
#+DATE:   2024-04-16
#+OPTIONS:  ^:nil H:5 num:t toc:2 \n:nil ::t |:t -:t f:t *:t tex:t d:(HIDE) tags:not-in-toc
#+STARTUP:  align nodlcheck oddeven lognotestate
#+SEQ_TODO: TODO(t) INPROGRESS(i) WAITING(w@) | DONE(d) CANCELED(c@)
#+LANGUAGE: en
#+TAGS:     noexport(n)
#+EXCLUDE_TAGS: noexport
#+FILETAGS: :tag1:tag2:note:ireader:



#+BEGIN_QUOTE
所有的写操作只追加而不修改老的数据。在 Bitcask 系统中，每个文件有一定的大小限制，当文件增加到相应的大小时，就会产生一个新的文件，老的文件只读不写。
#+END_QUOTE


#+BEGIN_QUOTE
存储系统的数据模型主要包括三类：文件、关系以及随着 NoSQL 技术流行起来的键值模型。
#+END_QUOTE


#+BEGIN_QUOTE
表格模型往往还支持无模式（schema-less）特性，也就是说，不需要预先定义每行包括哪些列以及每个列的类型，多行之间允许包含不同列。
#+END_QUOTE


#+BEGIN_QUOTE
NoSQL 系统带来了很多新的理念，比如良好的可扩展性，弱化数据库的设计范式，弱化一致性要
#+END_QUOTE


#+BEGIN_QUOTE
NoSQL 系统带来了很多新的理念，比如良好的可扩展性，弱化数据库的设计范式，弱化一致性要求，在一
#+END_QUOTE


#+BEGIN_QUOTE
NoSQL 系统带来了很多新的理念，比如良好的可扩展性，弱化数据库的设计范式，弱化一致性要求
#+END_QUOTE


#+BEGIN_QUOTE
需要将内存中的数据定期转储（Dump）到磁盘，这种技术称为 checkpoint（检查点）技术。系统定期将内存中的操作以某种易于加载的形式（checkpoint 文件）转储到磁盘中，并记录 checkpoint 时刻的日志回放点，以后故障恢复只需要回放 checkpoint 时刻的日志回放点之后的 REDO 日
#+END_QUOTE


#+BEGIN_QUOTE
需要将内存中的数据定期转储（Dump）到磁盘，这种技术称为 checkpoint（检查点）技术。系统定期将内存中的操作以某种易于加载的形式（checkpoint 文件）转储到磁盘中，并记录 checkpoint 时刻的日志回放点，以后故障恢复只需要回放
#+END_QUOTE


#+BEGIN_QUOTE
需要将内存中的数据定期转储（Dump）到磁盘，这种技术称为 checkpoint（检查点）技术。系统定期将内存中的操作以某种易于加载的形式（checkpoint 文件）转储到磁盘中，并记录 checkpoint 时刻的日志回放点，以后故障恢复只需要回放 checkpoint 时刻的日志回放点之后的 REDO 日志。 由于将内存数据转储到磁
#+END_QUOTE


#+BEGIN_QUOTE
需要将内存中的数据定期转储（Dump）到磁盘，这种技术称为 checkpoint（检查点）技术。系统定期将内存中的操作以某种易于加载的形式（checkpoint 文件）转储到磁盘中，并记录 checkpoint 时刻的日志回放点，以后故障恢复只需要回放 checkpoint 时刻的日志回放点之后的 REDO 日志。
#+END_QUOTE


#+BEGIN_QUOTE
存储系统在选择压缩算法时需要考虑压缩比和效率。读操作需要先读取磁盘中的内容再解压缩，写操作需要先压缩再将压缩结果写入到磁盘，整个操作的延时包括压缩/解压缩和磁盘读写的延迟，压缩比越大，磁盘读写的数据量越小，而压缩/解压缩的时间也会越长，所以这里需要一个很好的权衡点。Google Bigtable 系统中使用了 BMDiff 以及 Zippy 两种压缩算法，它们通过牺牲一定的压缩比换取算法执行速度的大幅提升，从而获得更好的折衷。
#+END_QUOTE


#+BEGIN_QUOTE
设计容错系统的一个基本原则是：网络永远是不可靠的，任何一个消息只有收到对方的回复后才可以认为发送成功，系统设计时总是假设网络将会出现异常并采取相应的处理措施。 （3）磁盘
#+END_QUOTE


#+BEGIN_QUOTE
设计容错系统的一个基本原则是：网络永远是不可靠的，任何一个消息只有收到对方的回复后才可以认为发送成功，系统设计时
#+END_QUOTE


#+BEGIN_QUOTE
设计容错系统的一个基本原则是：网络永远是不可靠的，任何一个消息只有收到对方的回复后才可以认为发送成功，系统设计时总是假设网络将会出现异常并采取相应的处理措施。
#+END_QUOTE


#+BEGIN_QUOTE
分布式存储系统需要能够自动识别负载高的节点，当某台机器的负载较高时，将它服务的部分数据迁移到其他机器，实现自动负载均衡。
#+END_QUOTE


#+BEGIN_QUOTE
分布式存储系统的一个基本要求就是透明性，包括数据分布透明性，数据迁移透明性，数据复制透明性，故障处理透明性
#+END_QUOTE


#+BEGIN_QUOTE
Paxos 协议用于保证同一个数据分片的多个副本之间的数据一致性
#+END_QUOTE


#+BEGIN_QUOTE
2PC 协议用于保证属于多个数据分片上的操作的原子性
#+END_QUOTE


#+BEGIN_QUOTE
Master 创建了一个 chunk，它会根据如下因素来选择 chunk 副本的初始位置：1）新副本所在的 ChunkServer 的磁盘利用率低于平均水平；2）限制每个 Chunk-Server“最近”创建的数量；3）每个 chunk 的所有副本不能在同一个机架
#+END_QUOTE


#+BEGIN_QUOTE
，在设计 GFS 时认为节点失效是常态，通过在软件层面进行故障检测，并且通过 chunk 复制操作将原有故障节点的服务迁移到新的节
#+END_QUOTE


#+BEGIN_QUOTE
，在设计 GFS 时认为节点失效是常态，
#+END_QUOTE


#+BEGIN_QUOTE
考虑到节点的异构性，不同节点的处理能力差别可能很大，Dynamo 使用了改进的一致性哈希算法：每个物理节点根据其性能的差异分配多个 token，每个 token 对应一个“虚拟节点”。每个虚拟节点的处理能力基本相当，并随机分布在哈希空间中。存储时，数据按照哈希值落到某个虚拟节点负责的
#+END_QUOTE


#+BEGIN_QUOTE
考虑到节点的异构性，不同节点的处理能力差别可能很大，Dynamo 使用了改进的一致性哈希算法：每个物理节点根据其性能的差异分配多个 token，每个 token 对应一个“虚拟节点”。每个虚拟节点的处理能力基本相当，并随机分布在哈希空间中。存储时，数据按照哈希值落到某个虚拟节点负责的区域，然后被存储在该虚拟节点所对应的物理节点中。
#+END_QUOTE


#+BEGIN_QUOTE
所有节点每隔固定时间（比如 1s）通过 Gossip 协议的方式从其他节点中任意选择一个与之通信的节点。如果连接成功，双方交换各自保存的集群信息。 Gossip 协议用于 P2P 系统中自治的节点协调对整个集群的认识，比如集群的节点状态、负载情
#+END_QUOTE


#+BEGIN_QUOTE
，Bigtable 的设计理念是构建在廉价的硬件之上，通过软件层面提供自动化容错和线性可扩展性能力。
#+END_QUOTE


#+BEGIN_QUOTE
整个系统设计时完全摒弃了随机写，除了操作日志总是顺序追加写入到普通 SAS 盘上，剩下的写请求都是对响应时间要求不是很高的批量顺序写，SSD 盘可以轻松应对，而大量查询请求的随机读，则发挥了 SSD 良好的随机读的特性。摒弃随机写，采用批量的顺序写，也使得固态盘的使用寿命不再成为问题，
#+END_QUOTE


#+BEGIN_QUOTE
内存管理是 C++高性能服务器的核心问题。一些通用的内存管理库，比如 Google TCMalloc，在内存申请/释放速度、小内存管理、锁开销等方面都已经做得相当卓越了，然而，我们并没有采用。这是因为，通用内存管理库在性能上毕竟不如专用的内存池
#+END_QUOTE


#+BEGIN_QUOTE
释放内存时，如果没有超出线程缓存的内存块个数限制，则将内存块还给线程局部的空闲链表
#+END_QUOTE


#+BEGIN_QUOTE
位锁（BitLock
#+END_QUOTE
